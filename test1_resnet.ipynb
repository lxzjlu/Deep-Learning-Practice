{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lxzjlu/Deep-Learning-Practice/blob/main/test1_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYXN_Au6_f6x"
      },
      "outputs": [],
      "source": [
        "import numpy as py\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zMLl1ENucU"
      },
      "source": [
        "超参数调整\n",
        "\n",
        "batch_size减小 使得运算速度逐渐变慢  但是模型测试精度有一定提升\n",
        "\n",
        "batch_size = 64\n",
        "Epoch = 20\n",
        "learning_rate = 0.01\n",
        "num_classes = 10\n",
        "训练精度 95.92%\n",
        "测试精度 65.50% 严重过拟合\n",
        "\n",
        "batch_size = 64\n",
        "Epoch = 20\n",
        "learning_rate = 0.01\n",
        "num_classes = 10\n",
        "训练精度 74.49%\n",
        "测试精度 67.45%\n",
        "\n",
        "batch_size = 16\n",
        "Epoch = 5\n",
        "learning_rate = 0.01\n",
        "num_classes = 10\n",
        "训练精度 72.94%\n",
        "测试精度 72.58%\n",
        "\n",
        "batch_size = 16\n",
        "Epoch = 10\n",
        "learning_rate = 0.01\n",
        "num_classes = 10\n",
        "训练精度 84.10%\n",
        "测试精度 75.80% 开始过拟合\n",
        "\n",
        "batch_size = 8\n",
        "Epoch = 10\n",
        "learning_rate = 0.01\n",
        "num_classes = 10\n",
        "训练精度 77.88%\n",
        "测试精度 72.12%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据集增强处理\n",
        "\n",
        "训练集 采用transform1 和 transform2 数据处理结果进行拼接 对测试集数据进行翻倍\n",
        "测试集 采用transform2 数据进行处理\n",
        "结果 训练精度85.11% 测试精度78.48%\n",
        "\n",
        "训练集 仅采用transform1 进行数据强化\n",
        "测试集 采用transform2 数据进行处理\n",
        "结果 训练精度 76.10% 测试精度 52.61%  ！！！问题问题问题（为什么这个的训练精度 远小于 没有数据增强的精度）\n",
        "\n",
        "训练集 采用transform1（增强版） 未翻倍\n",
        "测试集 采用transform2 数据进行处理\n",
        "结果 训练精度 90.79% 测试精度 76.27%（为什么图片变化大了，反而相同测试集上的测试精度没变）\n",
        "\n",
        "训练集 采用transform2 进行数据处理\n",
        "测试集 采用transform2 数据进行处理\n",
        "结果 训练精度 90.95% 测试精度 75.21%（发生严重过拟合）\n",
        "\n",
        "训练集 采用transform1（增强版） 和 transform2 数据处理结果进行拼接 对测试集数据进行翻倍\n",
        "测试集 采用transform2 数据进行处理\n",
        "结果 训练精度 74.06% 测试精度 75.69%   （learning_rate 0.01 考虑可能是太大了 调成0.005）\n",
        "\n",
        "训练集 采用transform1（增强版） 和 transform2 数据处理结果进行拼接 对测试集数据进行翻倍\n",
        "测试集 采用transform2 数据进行处理\n",
        "结果 训练精度 73.39% 测试精度 76.12%   （learning_rate 0.005 考虑可能是epoch太小了）\n",
        "\n",
        "训练集 采用transform1（增强版） 和 transform2 数据处理结果进行拼接 对测试集数据进行翻倍\n",
        "测试集 采用transform2 数据进行处理\n",
        "结果 训练精度  % 测试精度  %   （learning_rate 0.005 epoch = 25）\n",
        "\n"
      ],
      "metadata": {
        "id": "I3utts81kGVM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jnlhqBnAsMY"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "#超参数定义\n",
        "batch_size = 16\n",
        "Epoch = 25\n",
        "learning_rate = 0.005\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI757y1ZAvzw",
        "outputId": "ee34404a-730d-42b2-d041-f4c710d3501f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "##################################\n",
        "#输入数据处理   训练集 测试集\n",
        "\n",
        "#数据增强\n",
        "# transform1 = transforms.Compose([\n",
        "#     #图形尺寸填充 填充至36x36\n",
        "#     transforms.Pad(4),\n",
        "#     #随机水平翻转\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     #随机裁剪 裁剪至32x32\n",
        "#     transforms.RandomCrop(32),\n",
        "#     #转换至Tensor\n",
        "#     transforms.ToTensor(),\n",
        "#     ])\n",
        "#数据增强 完整版\n",
        "transform1 = transforms.Compose([\n",
        "    #图形尺寸填充 填充至36x36\n",
        "    transforms.Pad(4),\n",
        "    #随机水平翻转\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # 随机垂直翻转\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    # 随机旋转\n",
        "    transforms.RandomRotation(30),\n",
        "    #随机裁剪 裁剪至32x32\n",
        "    transforms.RandomCrop(32),\n",
        "    # 随机亮度调整\n",
        "    transforms.ColorJitter(brightness=0.2),\n",
        "    # 随机对比度调整\n",
        "    transforms.ColorJitter(contrast=0.2),\n",
        "    # 随机饱和度调整\n",
        "    transforms.ColorJitter(saturation=0.2),\n",
        "    # 随机色相调整\n",
        "    transforms.ColorJitter(hue=0.1),\n",
        "    #转换至Tensor\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "#数据处理\n",
        "transform2 = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#训练集数据导入 用两种方法实现数据集扩充\n",
        "trainset1 = torchvision.datasets.CIFAR10(root='./data',train=True,\n",
        "                     download=True, transform=transform1)\n",
        "trainset2 = torchvision.datasets.CIFAR10(root='./data',train=True,\n",
        "                     download=True, transform=transform2)\n",
        "concat_trainset = torch.utils.data.ConcatDataset([trainset1, trainset2])\n",
        "trainloader = torch.utils.data.DataLoader(concat_trainset, batch_size=batch_size,\n",
        "                     shuffle=True, num_workers=2)\n",
        "\n",
        "#训练集数据导入 仅用transform1 进行训练集数据增强\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data',train=True,\n",
        "#                      download=True, transform=transform2)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                      shuffle=True, num_workers=2)\n",
        "\n",
        "#测试集数据导入 仅采用正常数据处理方法\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                     download=True, transform=transform2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                     shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_9D_b6gAxaL"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "#训练集数据导入\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8zFHdGFA1AK"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "#网络定义Resnet\n",
        "\n",
        "#定义残差块\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):  #需要判断是否需要1×1的卷积\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                    kernel_size=3, padding=1, stride=strides)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                    kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                        kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X                                                 ###############################################可以去掉 不存在残差\n",
        "        return F.relu(Y)\n",
        "\n",
        "\n",
        "\n",
        "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(input_channels, num_channels,\n",
        "                                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels, num_channels))\n",
        "    return blk\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        self.b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "        self.b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "        self.b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "        self.b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
        "        self.linear = nn.Linear(512, 10)\n",
        "        self.Aavgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.b1(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.b4(x)\n",
        "        x = self.b5(x)\n",
        "        x = self.Aavgpool(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-G8HdojA5UJ"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "#定义损失函数 和 梯度计算\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meZk7DQ3A6wy",
        "outputId": "650c1894-defb-410b-bb1b-3f18d5c31a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Mon Jul 10 08:56:56 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    33W /  70W |   1373MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#GPU训练\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGCPlZ6cA86d",
        "outputId": "28a2604c-b924-4300-f838-a4c7c36d2f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Step: 200, Step_total: 6250, Loss:2.295\n",
            "Epoch: 1, Step: 400, Step_total: 6250, Loss:2.133\n",
            "Epoch: 1, Step: 600, Step_total: 6250, Loss:2.247\n",
            "Epoch: 1, Step: 800, Step_total: 6250, Loss:1.888\n",
            "Epoch: 1, Step: 1000, Step_total: 6250, Loss:2.256\n",
            "Epoch: 1, Step: 1200, Step_total: 6250, Loss:1.844\n",
            "Epoch: 1, Step: 1400, Step_total: 6250, Loss:1.753\n",
            "Epoch: 1, Step: 1600, Step_total: 6250, Loss:1.988\n",
            "Epoch: 1, Step: 1800, Step_total: 6250, Loss:1.247\n",
            "Epoch: 1, Step: 2000, Step_total: 6250, Loss:1.606\n",
            "Epoch: 1, Step: 2200, Step_total: 6250, Loss:1.886\n",
            "Epoch: 1, Step: 2400, Step_total: 6250, Loss:1.897\n",
            "Epoch: 1, Step: 2600, Step_total: 6250, Loss:1.638\n",
            "Epoch: 1, Step: 2800, Step_total: 6250, Loss:1.753\n",
            "Epoch: 1, Step: 3000, Step_total: 6250, Loss:1.797\n",
            "Epoch: 1, Step: 3200, Step_total: 6250, Loss:1.673\n",
            "Epoch: 1, Step: 3400, Step_total: 6250, Loss:1.808\n",
            "Epoch: 1, Step: 3600, Step_total: 6250, Loss:1.786\n",
            "Epoch: 1, Step: 3800, Step_total: 6250, Loss:1.772\n",
            "Epoch: 1, Step: 4000, Step_total: 6250, Loss:1.244\n",
            "Epoch: 1, Step: 4200, Step_total: 6250, Loss:1.212\n",
            "Epoch: 1, Step: 4400, Step_total: 6250, Loss:1.584\n",
            "Epoch: 1, Step: 4600, Step_total: 6250, Loss:1.474\n",
            "Epoch: 1, Step: 4800, Step_total: 6250, Loss:1.669\n",
            "Epoch: 1, Step: 5000, Step_total: 6250, Loss:1.152\n",
            "Epoch: 1, Step: 5200, Step_total: 6250, Loss:1.521\n",
            "Epoch: 1, Step: 5400, Step_total: 6250, Loss:1.525\n",
            "Epoch: 1, Step: 5600, Step_total: 6250, Loss:1.872\n",
            "Epoch: 1, Step: 5800, Step_total: 6250, Loss:1.118\n",
            "Epoch: 1, Step: 6000, Step_total: 6250, Loss:1.827\n",
            "Epoch: 1, Step: 6200, Step_total: 6250, Loss:1.323\n",
            "Epoch: 2, Step: 200, Step_total: 6250, Loss:1.143\n",
            "Epoch: 2, Step: 400, Step_total: 6250, Loss:1.439\n",
            "Epoch: 2, Step: 600, Step_total: 6250, Loss:1.501\n",
            "Epoch: 2, Step: 800, Step_total: 6250, Loss:1.343\n",
            "Epoch: 2, Step: 1000, Step_total: 6250, Loss:1.075\n",
            "Epoch: 2, Step: 1200, Step_total: 6250, Loss:1.260\n",
            "Epoch: 2, Step: 1400, Step_total: 6250, Loss:1.404\n",
            "Epoch: 2, Step: 1600, Step_total: 6250, Loss:1.202\n",
            "Epoch: 2, Step: 1800, Step_total: 6250, Loss:1.529\n",
            "Epoch: 2, Step: 2000, Step_total: 6250, Loss:1.632\n",
            "Epoch: 2, Step: 2200, Step_total: 6250, Loss:1.537\n",
            "Epoch: 2, Step: 2400, Step_total: 6250, Loss:1.599\n",
            "Epoch: 2, Step: 2600, Step_total: 6250, Loss:0.701\n",
            "Epoch: 2, Step: 2800, Step_total: 6250, Loss:1.334\n",
            "Epoch: 2, Step: 3000, Step_total: 6250, Loss:1.411\n",
            "Epoch: 2, Step: 3200, Step_total: 6250, Loss:1.160\n",
            "Epoch: 2, Step: 3400, Step_total: 6250, Loss:1.192\n",
            "Epoch: 2, Step: 3600, Step_total: 6250, Loss:1.087\n",
            "Epoch: 2, Step: 3800, Step_total: 6250, Loss:1.677\n",
            "Epoch: 2, Step: 4000, Step_total: 6250, Loss:1.216\n",
            "Epoch: 2, Step: 4200, Step_total: 6250, Loss:1.570\n",
            "Epoch: 2, Step: 4400, Step_total: 6250, Loss:1.320\n",
            "Epoch: 2, Step: 4600, Step_total: 6250, Loss:1.630\n",
            "Epoch: 2, Step: 4800, Step_total: 6250, Loss:1.060\n",
            "Epoch: 2, Step: 5000, Step_total: 6250, Loss:1.103\n",
            "Epoch: 2, Step: 5200, Step_total: 6250, Loss:1.481\n",
            "Epoch: 2, Step: 5400, Step_total: 6250, Loss:1.181\n",
            "Epoch: 2, Step: 5600, Step_total: 6250, Loss:0.879\n",
            "Epoch: 2, Step: 5800, Step_total: 6250, Loss:0.813\n",
            "Epoch: 2, Step: 6000, Step_total: 6250, Loss:1.219\n",
            "Epoch: 2, Step: 6200, Step_total: 6250, Loss:1.485\n",
            "Epoch: 3, Step: 200, Step_total: 6250, Loss:0.915\n",
            "Epoch: 3, Step: 400, Step_total: 6250, Loss:1.074\n",
            "Epoch: 3, Step: 600, Step_total: 6250, Loss:1.279\n",
            "Epoch: 3, Step: 800, Step_total: 6250, Loss:1.381\n",
            "Epoch: 3, Step: 1000, Step_total: 6250, Loss:0.767\n",
            "Epoch: 3, Step: 1200, Step_total: 6250, Loss:0.905\n",
            "Epoch: 3, Step: 1400, Step_total: 6250, Loss:1.905\n",
            "Epoch: 3, Step: 1600, Step_total: 6250, Loss:0.800\n",
            "Epoch: 3, Step: 1800, Step_total: 6250, Loss:1.079\n",
            "Epoch: 3, Step: 2000, Step_total: 6250, Loss:1.385\n",
            "Epoch: 3, Step: 2200, Step_total: 6250, Loss:1.739\n",
            "Epoch: 3, Step: 2400, Step_total: 6250, Loss:1.431\n",
            "Epoch: 3, Step: 2600, Step_total: 6250, Loss:1.571\n",
            "Epoch: 3, Step: 2800, Step_total: 6250, Loss:1.672\n",
            "Epoch: 3, Step: 3000, Step_total: 6250, Loss:0.630\n",
            "Epoch: 3, Step: 3200, Step_total: 6250, Loss:1.135\n",
            "Epoch: 3, Step: 3400, Step_total: 6250, Loss:0.925\n",
            "Epoch: 3, Step: 3600, Step_total: 6250, Loss:1.232\n",
            "Epoch: 3, Step: 3800, Step_total: 6250, Loss:0.989\n",
            "Epoch: 3, Step: 4000, Step_total: 6250, Loss:0.977\n",
            "Epoch: 3, Step: 4200, Step_total: 6250, Loss:0.886\n",
            "Epoch: 3, Step: 4400, Step_total: 6250, Loss:1.275\n",
            "Epoch: 3, Step: 4600, Step_total: 6250, Loss:0.993\n",
            "Epoch: 3, Step: 4800, Step_total: 6250, Loss:1.053\n",
            "Epoch: 3, Step: 5000, Step_total: 6250, Loss:1.744\n",
            "Epoch: 3, Step: 5200, Step_total: 6250, Loss:1.245\n",
            "Epoch: 3, Step: 5400, Step_total: 6250, Loss:1.041\n",
            "Epoch: 3, Step: 5600, Step_total: 6250, Loss:0.952\n",
            "Epoch: 3, Step: 5800, Step_total: 6250, Loss:1.991\n",
            "Epoch: 3, Step: 6000, Step_total: 6250, Loss:0.975\n",
            "Epoch: 3, Step: 6200, Step_total: 6250, Loss:0.832\n",
            "Epoch: 4, Step: 200, Step_total: 6250, Loss:0.755\n",
            "Epoch: 4, Step: 400, Step_total: 6250, Loss:1.125\n",
            "Epoch: 4, Step: 600, Step_total: 6250, Loss:1.753\n",
            "Epoch: 4, Step: 800, Step_total: 6250, Loss:1.093\n",
            "Epoch: 4, Step: 1000, Step_total: 6250, Loss:0.890\n",
            "Epoch: 4, Step: 1200, Step_total: 6250, Loss:0.684\n",
            "Epoch: 4, Step: 1400, Step_total: 6250, Loss:1.001\n",
            "Epoch: 4, Step: 1600, Step_total: 6250, Loss:1.412\n",
            "Epoch: 4, Step: 1800, Step_total: 6250, Loss:0.710\n",
            "Epoch: 4, Step: 2000, Step_total: 6250, Loss:1.469\n",
            "Epoch: 4, Step: 2200, Step_total: 6250, Loss:1.205\n",
            "Epoch: 4, Step: 2400, Step_total: 6250, Loss:1.482\n",
            "Epoch: 4, Step: 2600, Step_total: 6250, Loss:1.073\n",
            "Epoch: 4, Step: 2800, Step_total: 6250, Loss:0.621\n",
            "Epoch: 4, Step: 3000, Step_total: 6250, Loss:1.042\n",
            "Epoch: 4, Step: 3200, Step_total: 6250, Loss:0.725\n",
            "Epoch: 4, Step: 3400, Step_total: 6250, Loss:1.029\n",
            "Epoch: 4, Step: 3600, Step_total: 6250, Loss:1.544\n",
            "Epoch: 4, Step: 3800, Step_total: 6250, Loss:0.888\n",
            "Epoch: 4, Step: 4000, Step_total: 6250, Loss:0.881\n",
            "Epoch: 4, Step: 4200, Step_total: 6250, Loss:0.825\n",
            "Epoch: 4, Step: 4400, Step_total: 6250, Loss:0.853\n",
            "Epoch: 4, Step: 4600, Step_total: 6250, Loss:1.451\n",
            "Epoch: 4, Step: 4800, Step_total: 6250, Loss:0.772\n",
            "Epoch: 4, Step: 5000, Step_total: 6250, Loss:1.493\n",
            "Epoch: 4, Step: 5200, Step_total: 6250, Loss:0.946\n",
            "Epoch: 4, Step: 5400, Step_total: 6250, Loss:0.680\n",
            "Epoch: 4, Step: 5600, Step_total: 6250, Loss:1.566\n",
            "Epoch: 4, Step: 5800, Step_total: 6250, Loss:0.752\n",
            "Epoch: 4, Step: 6000, Step_total: 6250, Loss:0.810\n",
            "Epoch: 4, Step: 6200, Step_total: 6250, Loss:1.113\n",
            "Epoch: 5, Step: 200, Step_total: 6250, Loss:0.950\n",
            "Epoch: 5, Step: 400, Step_total: 6250, Loss:0.802\n",
            "Epoch: 5, Step: 600, Step_total: 6250, Loss:1.524\n",
            "Epoch: 5, Step: 800, Step_total: 6250, Loss:0.901\n",
            "Epoch: 5, Step: 1000, Step_total: 6250, Loss:1.016\n",
            "Epoch: 5, Step: 1200, Step_total: 6250, Loss:1.060\n",
            "Epoch: 5, Step: 1400, Step_total: 6250, Loss:0.917\n",
            "Epoch: 5, Step: 1600, Step_total: 6250, Loss:1.231\n",
            "Epoch: 5, Step: 1800, Step_total: 6250, Loss:1.088\n",
            "Epoch: 5, Step: 2000, Step_total: 6250, Loss:0.987\n",
            "Epoch: 5, Step: 2200, Step_total: 6250, Loss:0.730\n",
            "Epoch: 5, Step: 2400, Step_total: 6250, Loss:0.932\n",
            "Epoch: 5, Step: 2600, Step_total: 6250, Loss:0.922\n",
            "Epoch: 5, Step: 2800, Step_total: 6250, Loss:0.806\n",
            "Epoch: 5, Step: 3000, Step_total: 6250, Loss:0.681\n",
            "Epoch: 5, Step: 3200, Step_total: 6250, Loss:1.562\n",
            "Epoch: 5, Step: 3400, Step_total: 6250, Loss:1.444\n",
            "Epoch: 5, Step: 3600, Step_total: 6250, Loss:1.186\n",
            "Epoch: 5, Step: 3800, Step_total: 6250, Loss:0.741\n",
            "Epoch: 5, Step: 4000, Step_total: 6250, Loss:0.722\n",
            "Epoch: 5, Step: 4200, Step_total: 6250, Loss:1.349\n",
            "Epoch: 5, Step: 4400, Step_total: 6250, Loss:1.165\n",
            "Epoch: 5, Step: 4600, Step_total: 6250, Loss:0.924\n",
            "Epoch: 5, Step: 4800, Step_total: 6250, Loss:1.278\n",
            "Epoch: 5, Step: 5000, Step_total: 6250, Loss:1.134\n",
            "Epoch: 5, Step: 5200, Step_total: 6250, Loss:1.116\n",
            "Epoch: 5, Step: 5400, Step_total: 6250, Loss:0.873\n",
            "Epoch: 5, Step: 5600, Step_total: 6250, Loss:0.874\n",
            "Epoch: 5, Step: 5800, Step_total: 6250, Loss:1.082\n",
            "Epoch: 5, Step: 6000, Step_total: 6250, Loss:1.032\n",
            "Epoch: 5, Step: 6200, Step_total: 6250, Loss:0.788\n",
            "Epoch: 6, Step: 200, Step_total: 6250, Loss:0.859\n",
            "Epoch: 6, Step: 400, Step_total: 6250, Loss:0.879\n",
            "Epoch: 6, Step: 600, Step_total: 6250, Loss:0.358\n",
            "Epoch: 6, Step: 800, Step_total: 6250, Loss:0.971\n",
            "Epoch: 6, Step: 1000, Step_total: 6250, Loss:1.012\n",
            "Epoch: 6, Step: 1200, Step_total: 6250, Loss:0.761\n",
            "Epoch: 6, Step: 1400, Step_total: 6250, Loss:0.555\n",
            "Epoch: 6, Step: 1600, Step_total: 6250, Loss:0.685\n",
            "Epoch: 6, Step: 1800, Step_total: 6250, Loss:1.017\n",
            "Epoch: 6, Step: 2000, Step_total: 6250, Loss:1.244\n",
            "Epoch: 6, Step: 2200, Step_total: 6250, Loss:1.253\n",
            "Epoch: 6, Step: 2400, Step_total: 6250, Loss:0.813\n",
            "Epoch: 6, Step: 2600, Step_total: 6250, Loss:1.099\n",
            "Epoch: 6, Step: 2800, Step_total: 6250, Loss:0.421\n",
            "Epoch: 6, Step: 3000, Step_total: 6250, Loss:0.901\n",
            "Epoch: 6, Step: 3200, Step_total: 6250, Loss:1.194\n",
            "Epoch: 6, Step: 3400, Step_total: 6250, Loss:0.706\n",
            "Epoch: 6, Step: 3600, Step_total: 6250, Loss:0.733\n",
            "Epoch: 6, Step: 3800, Step_total: 6250, Loss:1.179\n",
            "Epoch: 6, Step: 4000, Step_total: 6250, Loss:1.617\n",
            "Epoch: 6, Step: 4200, Step_total: 6250, Loss:0.689\n",
            "Epoch: 6, Step: 4400, Step_total: 6250, Loss:0.846\n",
            "Epoch: 6, Step: 4600, Step_total: 6250, Loss:1.124\n",
            "Epoch: 6, Step: 4800, Step_total: 6250, Loss:0.959\n",
            "Epoch: 6, Step: 5000, Step_total: 6250, Loss:1.104\n",
            "Epoch: 6, Step: 5200, Step_total: 6250, Loss:0.822\n",
            "Epoch: 6, Step: 5400, Step_total: 6250, Loss:0.738\n",
            "Epoch: 6, Step: 5600, Step_total: 6250, Loss:0.888\n",
            "Epoch: 6, Step: 5800, Step_total: 6250, Loss:1.020\n",
            "Epoch: 6, Step: 6000, Step_total: 6250, Loss:1.198\n",
            "Epoch: 6, Step: 6200, Step_total: 6250, Loss:1.523\n",
            "Epoch: 7, Step: 200, Step_total: 6250, Loss:1.229\n",
            "Epoch: 7, Step: 400, Step_total: 6250, Loss:0.978\n",
            "Epoch: 7, Step: 600, Step_total: 6250, Loss:0.488\n",
            "Epoch: 7, Step: 800, Step_total: 6250, Loss:1.357\n",
            "Epoch: 7, Step: 1000, Step_total: 6250, Loss:1.069\n",
            "Epoch: 7, Step: 1200, Step_total: 6250, Loss:0.983\n",
            "Epoch: 7, Step: 1400, Step_total: 6250, Loss:0.651\n",
            "Epoch: 7, Step: 1600, Step_total: 6250, Loss:0.725\n",
            "Epoch: 7, Step: 1800, Step_total: 6250, Loss:0.658\n",
            "Epoch: 7, Step: 2000, Step_total: 6250, Loss:0.764\n",
            "Epoch: 7, Step: 2200, Step_total: 6250, Loss:1.068\n",
            "Epoch: 7, Step: 2400, Step_total: 6250, Loss:0.840\n",
            "Epoch: 7, Step: 2600, Step_total: 6250, Loss:0.482\n",
            "Epoch: 7, Step: 2800, Step_total: 6250, Loss:0.437\n",
            "Epoch: 7, Step: 3000, Step_total: 6250, Loss:0.582\n",
            "Epoch: 7, Step: 3200, Step_total: 6250, Loss:0.765\n",
            "Epoch: 7, Step: 3400, Step_total: 6250, Loss:0.621\n",
            "Epoch: 7, Step: 3600, Step_total: 6250, Loss:0.525\n",
            "Epoch: 7, Step: 3800, Step_total: 6250, Loss:0.598\n",
            "Epoch: 7, Step: 4000, Step_total: 6250, Loss:0.840\n",
            "Epoch: 7, Step: 4200, Step_total: 6250, Loss:1.127\n",
            "Epoch: 7, Step: 4400, Step_total: 6250, Loss:1.032\n",
            "Epoch: 7, Step: 4600, Step_total: 6250, Loss:0.996\n",
            "Epoch: 7, Step: 4800, Step_total: 6250, Loss:0.561\n",
            "Epoch: 7, Step: 5000, Step_total: 6250, Loss:0.731\n",
            "Epoch: 7, Step: 5200, Step_total: 6250, Loss:1.091\n",
            "Epoch: 7, Step: 5400, Step_total: 6250, Loss:1.311\n",
            "Epoch: 7, Step: 5600, Step_total: 6250, Loss:1.026\n",
            "Epoch: 7, Step: 5800, Step_total: 6250, Loss:0.854\n",
            "Epoch: 7, Step: 6000, Step_total: 6250, Loss:0.751\n",
            "Epoch: 7, Step: 6200, Step_total: 6250, Loss:0.412\n",
            "Epoch: 8, Step: 200, Step_total: 6250, Loss:0.655\n",
            "Epoch: 8, Step: 400, Step_total: 6250, Loss:0.365\n",
            "Epoch: 8, Step: 600, Step_total: 6250, Loss:0.901\n",
            "Epoch: 8, Step: 800, Step_total: 6250, Loss:1.021\n",
            "Epoch: 8, Step: 1000, Step_total: 6250, Loss:0.831\n",
            "Epoch: 8, Step: 1200, Step_total: 6250, Loss:0.638\n",
            "Epoch: 8, Step: 1400, Step_total: 6250, Loss:0.891\n",
            "Epoch: 8, Step: 1600, Step_total: 6250, Loss:0.456\n",
            "Epoch: 8, Step: 1800, Step_total: 6250, Loss:0.933\n",
            "Epoch: 8, Step: 2000, Step_total: 6250, Loss:0.520\n",
            "Epoch: 8, Step: 2200, Step_total: 6250, Loss:0.809\n",
            "Epoch: 8, Step: 2400, Step_total: 6250, Loss:0.954\n",
            "Epoch: 8, Step: 2600, Step_total: 6250, Loss:0.757\n",
            "Epoch: 8, Step: 2800, Step_total: 6250, Loss:1.039\n",
            "Epoch: 8, Step: 3000, Step_total: 6250, Loss:1.124\n",
            "Epoch: 8, Step: 3200, Step_total: 6250, Loss:0.628\n",
            "Epoch: 8, Step: 3400, Step_total: 6250, Loss:0.473\n",
            "Epoch: 8, Step: 3600, Step_total: 6250, Loss:0.534\n",
            "Epoch: 8, Step: 3800, Step_total: 6250, Loss:0.648\n",
            "Epoch: 8, Step: 4000, Step_total: 6250, Loss:1.024\n",
            "Epoch: 8, Step: 4200, Step_total: 6250, Loss:0.680\n",
            "Epoch: 8, Step: 4400, Step_total: 6250, Loss:0.842\n",
            "Epoch: 8, Step: 4600, Step_total: 6250, Loss:0.718\n",
            "Epoch: 8, Step: 4800, Step_total: 6250, Loss:0.782\n",
            "Epoch: 8, Step: 5000, Step_total: 6250, Loss:0.464\n",
            "Epoch: 8, Step: 5200, Step_total: 6250, Loss:1.029\n",
            "Epoch: 8, Step: 5400, Step_total: 6250, Loss:0.883\n",
            "Epoch: 8, Step: 5600, Step_total: 6250, Loss:0.522\n",
            "Epoch: 8, Step: 5800, Step_total: 6250, Loss:1.100\n",
            "Epoch: 8, Step: 6000, Step_total: 6250, Loss:0.935\n",
            "Epoch: 8, Step: 6200, Step_total: 6250, Loss:0.658\n",
            "Epoch: 9, Step: 200, Step_total: 6250, Loss:0.699\n",
            "Epoch: 9, Step: 400, Step_total: 6250, Loss:0.837\n",
            "Epoch: 9, Step: 600, Step_total: 6250, Loss:0.950\n",
            "Epoch: 9, Step: 800, Step_total: 6250, Loss:0.501\n",
            "Epoch: 9, Step: 1000, Step_total: 6250, Loss:1.146\n",
            "Epoch: 9, Step: 1200, Step_total: 6250, Loss:0.578\n",
            "Epoch: 9, Step: 1400, Step_total: 6250, Loss:1.228\n",
            "Epoch: 9, Step: 1600, Step_total: 6250, Loss:0.521\n",
            "Epoch: 9, Step: 1800, Step_total: 6250, Loss:0.785\n",
            "Epoch: 9, Step: 2000, Step_total: 6250, Loss:0.464\n",
            "Epoch: 9, Step: 2200, Step_total: 6250, Loss:0.527\n",
            "Epoch: 9, Step: 2400, Step_total: 6250, Loss:1.161\n",
            "Epoch: 9, Step: 2600, Step_total: 6250, Loss:0.661\n",
            "Epoch: 9, Step: 2800, Step_total: 6250, Loss:0.660\n",
            "Epoch: 9, Step: 3000, Step_total: 6250, Loss:0.754\n",
            "Epoch: 9, Step: 3200, Step_total: 6250, Loss:1.012\n",
            "Epoch: 9, Step: 3400, Step_total: 6250, Loss:0.804\n",
            "Epoch: 9, Step: 3600, Step_total: 6250, Loss:0.688\n",
            "Epoch: 9, Step: 3800, Step_total: 6250, Loss:0.881\n",
            "Epoch: 9, Step: 4000, Step_total: 6250, Loss:0.681\n",
            "Epoch: 9, Step: 4200, Step_total: 6250, Loss:0.816\n",
            "Epoch: 9, Step: 4400, Step_total: 6250, Loss:0.657\n",
            "Epoch: 9, Step: 4600, Step_total: 6250, Loss:0.968\n",
            "Epoch: 9, Step: 4800, Step_total: 6250, Loss:0.612\n",
            "Epoch: 9, Step: 5000, Step_total: 6250, Loss:0.873\n",
            "Epoch: 9, Step: 5200, Step_total: 6250, Loss:0.577\n",
            "Epoch: 9, Step: 5400, Step_total: 6250, Loss:1.195\n",
            "Epoch: 9, Step: 5600, Step_total: 6250, Loss:1.474\n",
            "Epoch: 9, Step: 5800, Step_total: 6250, Loss:0.440\n",
            "Epoch: 9, Step: 6000, Step_total: 6250, Loss:0.785\n",
            "Epoch: 9, Step: 6200, Step_total: 6250, Loss:0.827\n",
            "Epoch: 10, Step: 200, Step_total: 6250, Loss:0.689\n",
            "Epoch: 10, Step: 400, Step_total: 6250, Loss:0.667\n",
            "Epoch: 10, Step: 600, Step_total: 6250, Loss:1.913\n",
            "Epoch: 10, Step: 800, Step_total: 6250, Loss:0.791\n",
            "Epoch: 10, Step: 1000, Step_total: 6250, Loss:0.847\n",
            "Epoch: 10, Step: 1200, Step_total: 6250, Loss:0.575\n",
            "Epoch: 10, Step: 1400, Step_total: 6250, Loss:0.687\n",
            "Epoch: 10, Step: 1600, Step_total: 6250, Loss:0.753\n",
            "Epoch: 10, Step: 1800, Step_total: 6250, Loss:0.703\n",
            "Epoch: 10, Step: 2000, Step_total: 6250, Loss:0.413\n",
            "Epoch: 10, Step: 2200, Step_total: 6250, Loss:0.689\n",
            "Epoch: 10, Step: 2400, Step_total: 6250, Loss:0.666\n",
            "Epoch: 10, Step: 2600, Step_total: 6250, Loss:0.843\n",
            "Epoch: 10, Step: 2800, Step_total: 6250, Loss:0.543\n",
            "Epoch: 10, Step: 3000, Step_total: 6250, Loss:0.899\n",
            "Epoch: 10, Step: 3200, Step_total: 6250, Loss:0.604\n",
            "Epoch: 10, Step: 3400, Step_total: 6250, Loss:1.141\n",
            "Epoch: 10, Step: 3600, Step_total: 6250, Loss:0.651\n",
            "Epoch: 10, Step: 3800, Step_total: 6250, Loss:0.962\n",
            "Epoch: 10, Step: 4000, Step_total: 6250, Loss:0.945\n",
            "Epoch: 10, Step: 4200, Step_total: 6250, Loss:0.848\n",
            "Epoch: 10, Step: 4400, Step_total: 6250, Loss:0.814\n",
            "Epoch: 10, Step: 4600, Step_total: 6250, Loss:0.624\n",
            "Epoch: 10, Step: 4800, Step_total: 6250, Loss:0.447\n",
            "Epoch: 10, Step: 5000, Step_total: 6250, Loss:1.284\n",
            "Epoch: 10, Step: 5200, Step_total: 6250, Loss:1.301\n",
            "Epoch: 10, Step: 5400, Step_total: 6250, Loss:0.380\n",
            "Epoch: 10, Step: 5600, Step_total: 6250, Loss:0.702\n",
            "Epoch: 10, Step: 5800, Step_total: 6250, Loss:1.016\n",
            "Epoch: 10, Step: 6000, Step_total: 6250, Loss:0.701\n",
            "Epoch: 10, Step: 6200, Step_total: 6250, Loss:0.783\n",
            "Epoch: 11, Step: 200, Step_total: 6250, Loss:0.537\n",
            "Epoch: 11, Step: 400, Step_total: 6250, Loss:0.582\n",
            "Epoch: 11, Step: 600, Step_total: 6250, Loss:0.542\n",
            "Epoch: 11, Step: 800, Step_total: 6250, Loss:0.383\n",
            "Epoch: 11, Step: 1000, Step_total: 6250, Loss:0.902\n",
            "Epoch: 11, Step: 1200, Step_total: 6250, Loss:1.128\n",
            "Epoch: 11, Step: 1400, Step_total: 6250, Loss:0.806\n",
            "Epoch: 11, Step: 1600, Step_total: 6250, Loss:0.367\n",
            "Epoch: 11, Step: 1800, Step_total: 6250, Loss:0.461\n",
            "Epoch: 11, Step: 2000, Step_total: 6250, Loss:0.987\n",
            "Epoch: 11, Step: 2200, Step_total: 6250, Loss:0.870\n",
            "Epoch: 11, Step: 2400, Step_total: 6250, Loss:0.617\n",
            "Epoch: 11, Step: 2600, Step_total: 6250, Loss:0.484\n",
            "Epoch: 11, Step: 2800, Step_total: 6250, Loss:0.627\n",
            "Epoch: 11, Step: 3000, Step_total: 6250, Loss:0.891\n",
            "Epoch: 11, Step: 3200, Step_total: 6250, Loss:1.024\n",
            "Epoch: 11, Step: 3400, Step_total: 6250, Loss:0.764\n",
            "Epoch: 11, Step: 3600, Step_total: 6250, Loss:0.736\n",
            "Epoch: 11, Step: 3800, Step_total: 6250, Loss:0.773\n",
            "Epoch: 11, Step: 4000, Step_total: 6250, Loss:0.272\n",
            "Epoch: 11, Step: 4200, Step_total: 6250, Loss:0.558\n",
            "Epoch: 11, Step: 4400, Step_total: 6250, Loss:0.529\n",
            "Epoch: 11, Step: 4600, Step_total: 6250, Loss:0.864\n",
            "Epoch: 11, Step: 4800, Step_total: 6250, Loss:1.205\n",
            "Epoch: 11, Step: 5000, Step_total: 6250, Loss:0.810\n",
            "Epoch: 11, Step: 5200, Step_total: 6250, Loss:1.138\n",
            "Epoch: 11, Step: 5400, Step_total: 6250, Loss:0.637\n",
            "Epoch: 11, Step: 5600, Step_total: 6250, Loss:0.680\n",
            "Epoch: 11, Step: 5800, Step_total: 6250, Loss:1.070\n",
            "Epoch: 11, Step: 6000, Step_total: 6250, Loss:0.798\n",
            "Epoch: 11, Step: 6200, Step_total: 6250, Loss:0.573\n",
            "Epoch: 12, Step: 200, Step_total: 6250, Loss:0.377\n",
            "Epoch: 12, Step: 400, Step_total: 6250, Loss:0.570\n",
            "Epoch: 12, Step: 600, Step_total: 6250, Loss:0.914\n",
            "Epoch: 12, Step: 800, Step_total: 6250, Loss:0.864\n",
            "Epoch: 12, Step: 1000, Step_total: 6250, Loss:0.900\n",
            "Epoch: 12, Step: 1200, Step_total: 6250, Loss:0.793\n",
            "Epoch: 12, Step: 1400, Step_total: 6250, Loss:0.735\n",
            "Epoch: 12, Step: 1600, Step_total: 6250, Loss:0.573\n",
            "Epoch: 12, Step: 1800, Step_total: 6250, Loss:0.456\n",
            "Epoch: 12, Step: 2000, Step_total: 6250, Loss:0.265\n",
            "Epoch: 12, Step: 2200, Step_total: 6250, Loss:1.009\n",
            "Epoch: 12, Step: 2400, Step_total: 6250, Loss:0.491\n",
            "Epoch: 12, Step: 2600, Step_total: 6250, Loss:0.597\n",
            "Epoch: 12, Step: 2800, Step_total: 6250, Loss:0.972\n",
            "Epoch: 12, Step: 3000, Step_total: 6250, Loss:0.873\n",
            "Epoch: 12, Step: 3200, Step_total: 6250, Loss:0.542\n",
            "Epoch: 12, Step: 3400, Step_total: 6250, Loss:0.689\n",
            "Epoch: 12, Step: 3600, Step_total: 6250, Loss:0.631\n",
            "Epoch: 12, Step: 3800, Step_total: 6250, Loss:0.334\n",
            "Epoch: 12, Step: 4000, Step_total: 6250, Loss:0.595\n",
            "Epoch: 12, Step: 4200, Step_total: 6250, Loss:0.701\n",
            "Epoch: 12, Step: 4400, Step_total: 6250, Loss:0.332\n",
            "Epoch: 12, Step: 4600, Step_total: 6250, Loss:0.515\n",
            "Epoch: 12, Step: 4800, Step_total: 6250, Loss:0.959\n",
            "Epoch: 12, Step: 5000, Step_total: 6250, Loss:0.591\n",
            "Epoch: 12, Step: 5200, Step_total: 6250, Loss:1.040\n",
            "Epoch: 12, Step: 5400, Step_total: 6250, Loss:0.455\n",
            "Epoch: 12, Step: 5600, Step_total: 6250, Loss:0.743\n",
            "Epoch: 12, Step: 5800, Step_total: 6250, Loss:0.719\n",
            "Epoch: 12, Step: 6000, Step_total: 6250, Loss:0.745\n",
            "Epoch: 12, Step: 6200, Step_total: 6250, Loss:0.614\n",
            "Epoch: 13, Step: 200, Step_total: 6250, Loss:1.097\n",
            "Epoch: 13, Step: 400, Step_total: 6250, Loss:0.631\n",
            "Epoch: 13, Step: 600, Step_total: 6250, Loss:0.563\n",
            "Epoch: 13, Step: 800, Step_total: 6250, Loss:0.502\n",
            "Epoch: 13, Step: 1000, Step_total: 6250, Loss:0.562\n",
            "Epoch: 13, Step: 1200, Step_total: 6250, Loss:0.589\n",
            "Epoch: 13, Step: 1400, Step_total: 6250, Loss:0.493\n",
            "Epoch: 13, Step: 1600, Step_total: 6250, Loss:0.921\n",
            "Epoch: 13, Step: 1800, Step_total: 6250, Loss:0.884\n",
            "Epoch: 13, Step: 2000, Step_total: 6250, Loss:0.191\n",
            "Epoch: 13, Step: 2200, Step_total: 6250, Loss:0.559\n",
            "Epoch: 13, Step: 2400, Step_total: 6250, Loss:1.011\n",
            "Epoch: 13, Step: 2600, Step_total: 6250, Loss:0.931\n",
            "Epoch: 13, Step: 2800, Step_total: 6250, Loss:0.743\n",
            "Epoch: 13, Step: 3000, Step_total: 6250, Loss:0.504\n",
            "Epoch: 13, Step: 3200, Step_total: 6250, Loss:0.932\n",
            "Epoch: 13, Step: 3400, Step_total: 6250, Loss:1.179\n",
            "Epoch: 13, Step: 3600, Step_total: 6250, Loss:0.907\n",
            "Epoch: 13, Step: 3800, Step_total: 6250, Loss:0.957\n",
            "Epoch: 13, Step: 4000, Step_total: 6250, Loss:0.516\n",
            "Epoch: 13, Step: 4200, Step_total: 6250, Loss:0.486\n",
            "Epoch: 13, Step: 4400, Step_total: 6250, Loss:0.645\n",
            "Epoch: 13, Step: 4600, Step_total: 6250, Loss:0.440\n",
            "Epoch: 13, Step: 4800, Step_total: 6250, Loss:0.645\n",
            "Epoch: 13, Step: 5000, Step_total: 6250, Loss:0.784\n",
            "Epoch: 13, Step: 5200, Step_total: 6250, Loss:0.791\n",
            "Epoch: 13, Step: 5400, Step_total: 6250, Loss:0.630\n",
            "Epoch: 13, Step: 5600, Step_total: 6250, Loss:0.510\n",
            "Epoch: 13, Step: 5800, Step_total: 6250, Loss:0.683\n",
            "Epoch: 13, Step: 6000, Step_total: 6250, Loss:1.157\n",
            "Epoch: 13, Step: 6200, Step_total: 6250, Loss:0.489\n",
            "Epoch: 14, Step: 200, Step_total: 6250, Loss:0.703\n",
            "Epoch: 14, Step: 400, Step_total: 6250, Loss:0.729\n",
            "Epoch: 14, Step: 600, Step_total: 6250, Loss:0.629\n",
            "Epoch: 14, Step: 800, Step_total: 6250, Loss:0.394\n",
            "Epoch: 14, Step: 1000, Step_total: 6250, Loss:1.042\n",
            "Epoch: 14, Step: 1200, Step_total: 6250, Loss:0.782\n",
            "Epoch: 14, Step: 1400, Step_total: 6250, Loss:0.474\n",
            "Epoch: 14, Step: 1600, Step_total: 6250, Loss:0.852\n",
            "Epoch: 14, Step: 1800, Step_total: 6250, Loss:0.600\n",
            "Epoch: 14, Step: 2000, Step_total: 6250, Loss:1.104\n",
            "Epoch: 14, Step: 2200, Step_total: 6250, Loss:0.972\n",
            "Epoch: 14, Step: 2400, Step_total: 6250, Loss:0.465\n",
            "Epoch: 14, Step: 2600, Step_total: 6250, Loss:0.653\n",
            "Epoch: 14, Step: 2800, Step_total: 6250, Loss:0.420\n",
            "Epoch: 14, Step: 3000, Step_total: 6250, Loss:0.499\n",
            "Epoch: 14, Step: 3200, Step_total: 6250, Loss:0.928\n",
            "Epoch: 14, Step: 3400, Step_total: 6250, Loss:0.441\n",
            "Epoch: 14, Step: 3600, Step_total: 6250, Loss:0.831\n",
            "Epoch: 14, Step: 3800, Step_total: 6250, Loss:0.730\n",
            "Epoch: 14, Step: 4000, Step_total: 6250, Loss:0.689\n",
            "Epoch: 14, Step: 4200, Step_total: 6250, Loss:0.772\n",
            "Epoch: 14, Step: 4400, Step_total: 6250, Loss:0.409\n",
            "Epoch: 14, Step: 4600, Step_total: 6250, Loss:0.781\n",
            "Epoch: 14, Step: 4800, Step_total: 6250, Loss:0.930\n",
            "Epoch: 14, Step: 5000, Step_total: 6250, Loss:0.299\n",
            "Epoch: 14, Step: 5200, Step_total: 6250, Loss:0.285\n",
            "Epoch: 14, Step: 5400, Step_total: 6250, Loss:0.808\n",
            "Epoch: 14, Step: 5600, Step_total: 6250, Loss:0.966\n",
            "Epoch: 14, Step: 5800, Step_total: 6250, Loss:0.716\n",
            "Epoch: 14, Step: 6000, Step_total: 6250, Loss:0.966\n",
            "Epoch: 14, Step: 6200, Step_total: 6250, Loss:0.402\n",
            "Epoch: 15, Step: 200, Step_total: 6250, Loss:0.373\n",
            "Epoch: 15, Step: 400, Step_total: 6250, Loss:0.807\n",
            "Epoch: 15, Step: 600, Step_total: 6250, Loss:0.735\n",
            "Epoch: 15, Step: 800, Step_total: 6250, Loss:0.763\n",
            "Epoch: 15, Step: 1000, Step_total: 6250, Loss:0.336\n",
            "Epoch: 15, Step: 1200, Step_total: 6250, Loss:1.027\n",
            "Epoch: 15, Step: 1400, Step_total: 6250, Loss:0.571\n",
            "Epoch: 15, Step: 1600, Step_total: 6250, Loss:0.895\n",
            "Epoch: 15, Step: 1800, Step_total: 6250, Loss:0.277\n",
            "Epoch: 15, Step: 2000, Step_total: 6250, Loss:0.808\n",
            "Epoch: 15, Step: 2200, Step_total: 6250, Loss:0.487\n",
            "Epoch: 15, Step: 2400, Step_total: 6250, Loss:0.555\n",
            "Epoch: 15, Step: 2600, Step_total: 6250, Loss:0.415\n",
            "Epoch: 15, Step: 2800, Step_total: 6250, Loss:0.896\n",
            "Epoch: 15, Step: 3000, Step_total: 6250, Loss:0.491\n",
            "Epoch: 15, Step: 3200, Step_total: 6250, Loss:1.110\n",
            "Epoch: 15, Step: 3400, Step_total: 6250, Loss:0.565\n",
            "Epoch: 15, Step: 3600, Step_total: 6250, Loss:0.158\n",
            "Epoch: 15, Step: 3800, Step_total: 6250, Loss:0.946\n",
            "Epoch: 15, Step: 4000, Step_total: 6250, Loss:0.847\n",
            "Epoch: 15, Step: 4200, Step_total: 6250, Loss:0.335\n",
            "Epoch: 15, Step: 4400, Step_total: 6250, Loss:0.538\n",
            "Epoch: 15, Step: 4600, Step_total: 6250, Loss:0.293\n",
            "Epoch: 15, Step: 4800, Step_total: 6250, Loss:1.152\n",
            "Epoch: 15, Step: 5000, Step_total: 6250, Loss:0.694\n",
            "Epoch: 15, Step: 5200, Step_total: 6250, Loss:0.454\n",
            "Epoch: 15, Step: 5400, Step_total: 6250, Loss:0.530\n",
            "Epoch: 15, Step: 5600, Step_total: 6250, Loss:0.665\n",
            "Epoch: 15, Step: 5800, Step_total: 6250, Loss:0.358\n",
            "Epoch: 15, Step: 6000, Step_total: 6250, Loss:0.568\n",
            "Epoch: 15, Step: 6200, Step_total: 6250, Loss:0.393\n",
            "Epoch: 16, Step: 200, Step_total: 6250, Loss:0.684\n",
            "Epoch: 16, Step: 400, Step_total: 6250, Loss:0.789\n",
            "Epoch: 16, Step: 600, Step_total: 6250, Loss:0.599\n",
            "Epoch: 16, Step: 800, Step_total: 6250, Loss:0.407\n",
            "Epoch: 16, Step: 1000, Step_total: 6250, Loss:0.645\n",
            "Epoch: 16, Step: 1200, Step_total: 6250, Loss:0.349\n",
            "Epoch: 16, Step: 1400, Step_total: 6250, Loss:0.437\n",
            "Epoch: 16, Step: 1600, Step_total: 6250, Loss:0.424\n",
            "Epoch: 16, Step: 1800, Step_total: 6250, Loss:0.650\n",
            "Epoch: 16, Step: 2000, Step_total: 6250, Loss:0.463\n",
            "Epoch: 16, Step: 2200, Step_total: 6250, Loss:0.516\n",
            "Epoch: 16, Step: 2400, Step_total: 6250, Loss:0.465\n",
            "Epoch: 16, Step: 2600, Step_total: 6250, Loss:0.268\n",
            "Epoch: 16, Step: 2800, Step_total: 6250, Loss:1.078\n",
            "Epoch: 16, Step: 3000, Step_total: 6250, Loss:0.710\n",
            "Epoch: 16, Step: 3200, Step_total: 6250, Loss:0.619\n",
            "Epoch: 16, Step: 3400, Step_total: 6250, Loss:0.498\n",
            "Epoch: 16, Step: 3600, Step_total: 6250, Loss:0.375\n",
            "Epoch: 16, Step: 3800, Step_total: 6250, Loss:0.493\n",
            "Epoch: 16, Step: 4000, Step_total: 6250, Loss:1.106\n",
            "Epoch: 16, Step: 4200, Step_total: 6250, Loss:0.984\n",
            "Epoch: 16, Step: 4400, Step_total: 6250, Loss:0.941\n",
            "Epoch: 16, Step: 4600, Step_total: 6250, Loss:0.772\n",
            "Epoch: 16, Step: 4800, Step_total: 6250, Loss:0.274\n",
            "Epoch: 16, Step: 5000, Step_total: 6250, Loss:0.830\n",
            "Epoch: 16, Step: 5200, Step_total: 6250, Loss:1.414\n",
            "Epoch: 16, Step: 5400, Step_total: 6250, Loss:0.351\n",
            "Epoch: 16, Step: 5600, Step_total: 6250, Loss:0.572\n",
            "Epoch: 16, Step: 5800, Step_total: 6250, Loss:0.760\n",
            "Epoch: 16, Step: 6000, Step_total: 6250, Loss:0.806\n",
            "Epoch: 16, Step: 6200, Step_total: 6250, Loss:0.637\n",
            "Epoch: 17, Step: 200, Step_total: 6250, Loss:0.699\n",
            "Epoch: 17, Step: 400, Step_total: 6250, Loss:0.345\n",
            "Epoch: 17, Step: 600, Step_total: 6250, Loss:0.877\n",
            "Epoch: 17, Step: 800, Step_total: 6250, Loss:0.476\n",
            "Epoch: 17, Step: 1000, Step_total: 6250, Loss:0.376\n",
            "Epoch: 17, Step: 1200, Step_total: 6250, Loss:0.478\n",
            "Epoch: 17, Step: 1400, Step_total: 6250, Loss:0.882\n",
            "Epoch: 17, Step: 1600, Step_total: 6250, Loss:0.695\n",
            "Epoch: 17, Step: 1800, Step_total: 6250, Loss:0.359\n",
            "Epoch: 17, Step: 2000, Step_total: 6250, Loss:0.238\n",
            "Epoch: 17, Step: 2200, Step_total: 6250, Loss:0.396\n",
            "Epoch: 17, Step: 2400, Step_total: 6250, Loss:0.459\n",
            "Epoch: 17, Step: 2600, Step_total: 6250, Loss:0.525\n",
            "Epoch: 17, Step: 2800, Step_total: 6250, Loss:0.821\n",
            "Epoch: 17, Step: 3000, Step_total: 6250, Loss:0.539\n",
            "Epoch: 17, Step: 3200, Step_total: 6250, Loss:1.029\n",
            "Epoch: 17, Step: 3400, Step_total: 6250, Loss:0.561\n",
            "Epoch: 17, Step: 3600, Step_total: 6250, Loss:0.665\n",
            "Epoch: 17, Step: 3800, Step_total: 6250, Loss:0.738\n",
            "Epoch: 17, Step: 4000, Step_total: 6250, Loss:0.520\n",
            "Epoch: 17, Step: 4200, Step_total: 6250, Loss:0.329\n",
            "Epoch: 17, Step: 4400, Step_total: 6250, Loss:1.078\n",
            "Epoch: 17, Step: 4600, Step_total: 6250, Loss:0.750\n",
            "Epoch: 17, Step: 4800, Step_total: 6250, Loss:1.249\n",
            "Epoch: 17, Step: 5000, Step_total: 6250, Loss:0.377\n",
            "Epoch: 17, Step: 5200, Step_total: 6250, Loss:0.558\n",
            "Epoch: 17, Step: 5400, Step_total: 6250, Loss:0.554\n",
            "Epoch: 17, Step: 5600, Step_total: 6250, Loss:0.604\n",
            "Epoch: 17, Step: 5800, Step_total: 6250, Loss:0.561\n",
            "Epoch: 17, Step: 6000, Step_total: 6250, Loss:0.861\n",
            "Epoch: 17, Step: 6200, Step_total: 6250, Loss:0.254\n",
            "Epoch: 18, Step: 200, Step_total: 6250, Loss:0.593\n",
            "Epoch: 18, Step: 400, Step_total: 6250, Loss:0.932\n",
            "Epoch: 18, Step: 600, Step_total: 6250, Loss:0.491\n",
            "Epoch: 18, Step: 800, Step_total: 6250, Loss:0.353\n",
            "Epoch: 18, Step: 1000, Step_total: 6250, Loss:0.309\n",
            "Epoch: 18, Step: 1200, Step_total: 6250, Loss:0.474\n",
            "Epoch: 18, Step: 1400, Step_total: 6250, Loss:0.298\n",
            "Epoch: 18, Step: 1600, Step_total: 6250, Loss:0.758\n",
            "Epoch: 18, Step: 1800, Step_total: 6250, Loss:1.284\n",
            "Epoch: 18, Step: 2000, Step_total: 6250, Loss:0.423\n",
            "Epoch: 18, Step: 2200, Step_total: 6250, Loss:0.705\n",
            "Epoch: 18, Step: 2400, Step_total: 6250, Loss:0.818\n",
            "Epoch: 18, Step: 2600, Step_total: 6250, Loss:0.304\n",
            "Epoch: 18, Step: 2800, Step_total: 6250, Loss:0.703\n",
            "Epoch: 18, Step: 3000, Step_total: 6250, Loss:0.795\n",
            "Epoch: 18, Step: 3200, Step_total: 6250, Loss:0.671\n",
            "Epoch: 18, Step: 3400, Step_total: 6250, Loss:0.432\n"
          ]
        }
      ],
      "source": [
        "###########################\n",
        "#训练过程\n",
        "\n",
        "#GPU训练\n",
        "#net.to(device)\n",
        "net = net.cuda()\n",
        "#存储损失与精度\n",
        "loss_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "\n",
        "for epoch in range(Epoch):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        #获取输入\n",
        "\n",
        "        inputs, labels = data\n",
        "        #inputs.to(device)\n",
        "        inputs = inputs.cuda()\n",
        "        #labels.to(device)\n",
        "        labels = labels.cuda()\n",
        "        #梯度置零\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #前向计算\n",
        "        outputs = net(inputs)\n",
        "        #获取损失\n",
        "        loss = criterion(outputs, labels)\n",
        "        #计算梯度\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #累计损失\n",
        "        running_loss += loss.item()\n",
        "        # 计算准确率\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "        #每200次循环输出打印一次\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print('Epoch: {}, Step: {}, Step_total: {}, Loss:{:.3f}'.format(epoch+1, i+1, len(trainloader), loss.item()))\n",
        "\n",
        "    #计算各epoch中的平均损失值和准确率\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    accuracy = 100.0 * correct_train / total_train\n",
        "    #存储平均损失值和准确率\n",
        "    loss_history.append(avg_loss)\n",
        "    accuracy_history.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quZCO0utA_if"
      },
      "outputs": [],
      "source": [
        "###############################\n",
        "#绘制损失和准确率曲线\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(Epoch), loss_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(Epoch), accuracy_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQfCu3-hBCwu"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "#测试\n",
        "net.eval()\n",
        "#总精度及各类精度相关参数定义\n",
        "correct = 0\n",
        "total = 0\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "#测试数据集运行\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = net(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "    for label, prediction in zip(labels, predicted):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "print('Accuracy of the net on the train iamges is {:.2f} %'.format(accuracy_history[-1]))\n",
        "print('Accuracy of the net on the test iamges is {:.2f} %'.format(100 * correct / total))\n",
        "print('\\n')\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elgkOQ2VBFpq"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "#可视化\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "#打印测试集照片\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "#输出真实标签\n",
        "print('Truth Labels: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
        "\n",
        "#输出预测标签\n",
        "images = images.cuda()\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "print('Predicted Labels: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(batch_size)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZULdGQWxJwrvqVtSgVCOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}